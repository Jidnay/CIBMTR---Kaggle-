{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c3a9995-acad-49bc-a3cb-8e8d9e3d59db",
   "metadata": {},
   "source": [
    "# Kaggle: CIBMTR - Equity in post-HCT Survival Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c34ef-c548-4350-b1c2-ab5a5eead056",
   "metadata": {},
   "source": [
    "Final Training Notebook of Kaggle CIBMTR competition\n",
    "\n",
    "Some of the code where from Chris Deotte Baseline Notebook:\n",
    "https://www.kaggle.com/code/cdeotte/gpu-lightgbm-baseline-cv-681-lb-685#XGBoost-with-Survival:Cox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e542e8",
   "metadata": {
    "papermill": {
     "duration": 0.006787,
     "end_time": "2024-12-09T00:45:34.699566",
     "exception": false,
     "start_time": "2024-12-09T00:45:34.692779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f1e5c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 1.093687,
     "end_time": "2024-12-09T00:45:35.800042",
     "exception": false,
     "start_time": "2024-12-09T00:45:34.706355",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from metric import score\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import gamma\n",
    "from lifelines import KaplanMeierFitter, NelsonAalenFitter\n",
    "from scipy.stats import rankdata \n",
    "import joblib\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "path = \"/kaggle/input/\"\n",
    "path = \"\"\n",
    "\n",
    "# Load dictionary\n",
    "with open(\"category_mappings.pkl\", \"rb\") as file:\n",
    "    category_mappings = pickle.load(file)\n",
    "    \n",
    "test = pd.read_csv(\"equity-post-HCT-survival-predictions/test.csv\")\n",
    "print(\"Test shape:\", test.shape )\n",
    "\n",
    "train = pd.read_csv(\"equity-post-HCT-survival-predictions/train.csv\")\n",
    "print(\"Train shape:\",train.shape)\n",
    "\n",
    "train.head()\n",
    "\n",
    "# Suppress only FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e591f-c0a1-4722-9143-03ce3394a964",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d26b9e5-2b2c-4267-90d0-f44fb171fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dri_mapping = {\n",
    "    \"Very high\": 3.2,\n",
    "    \"High\": 2.2,\n",
    "    \"High - TED AML case <missing cytogenetics\": 2.8,\n",
    "    \"Intermediate\": 1.4,\n",
    "    \"Intermediate - TED AML case <missing cytogenetics\": 1.7,\n",
    "    \"Low\": 0.9,\n",
    "    \"TBD cytogenetics\": 0.8,\n",
    "    \"N/A - non-malignant indication\": 0.4,\n",
    "    \"N/A - disease not classifiable\": 0.3,\n",
    "    \"N/A - pediatric\": 0.5,\n",
    "    \"Missing disease status\": 0\n",
    "}\n",
    "\n",
    "sex_match_mapping = {\n",
    "    'M-F': 1, \n",
    "    'F-F': 0.5,\n",
    "    'F-M': 0.5,\n",
    "    'M-M':1\n",
    "}\n",
    "\n",
    "cyto_mapping = {\n",
    "    \"Favorable\": 4,\n",
    "    \"Intermediate\": 3,\n",
    "    \"Poor\": 2,\n",
    "    \"Normal\": 1,\n",
    "    \"Other\": 0,\n",
    "    \"Not tested\": -1,\n",
    "    \"TBD\": -1\n",
    "}\n",
    "\n",
    "cyto_detail_mapping = {\n",
    "    \"Favorable\": 4,\n",
    "    \"Intermediate\": 3,\n",
    "    \"Poor\": 2,\n",
    "    \"Not tested\": -1,\n",
    "    \"TBD\": -1\n",
    "}\n",
    "\n",
    "tbi_mapping = {\n",
    "    \"No TBI\": 0,\n",
    "    \"TBI + Cy +- Other\": 0.3,\n",
    "    \"TBI +- Other, <=cGy\": 0.4,  # Lower dose, lower impact\n",
    "    \"TBI +- Other, >cGy\": 0.5,   # Higher dose, higher impact\n",
    "    \"TBI +- Other, -cGy, single\": 0.4,  # Similar to >cGy\n",
    "    \"TBI +- Other, -cGy, fractionated\": 0.2,  # Lower risk due to fractionation\n",
    "    \"TBI +- Other, -cGy, unknown dose\": 0.5,  # Slightly increased risk\n",
    "    \"TBI +- Other, unknown dose\": 0.6  # Highest risk (uncertain)\n",
    "}\n",
    "\n",
    "\n",
    "conditioning_mapping = {\n",
    "    \"MAC\": 4,  # Most intense\n",
    "    \"RIC\": 3,  # Intermediate intensity\n",
    "    \"NMA\": 2,  # Least intense\n",
    "    \"TBD\": 0.8,  # Unknown\n",
    "    \"No drugs reported\": 0,  # No conditioning\n",
    "    \"N/A, F(pre-TED) not submitted\": 0.5,  # Missing data\n",
    "}\n",
    "\n",
    "disease_mapping = {\n",
    "    \"ALL\": \"1\",  # Acute Lymphoblastic Leukemia\n",
    "    \"AML\": \"1\",  # Acute Myeloid Leukemia\n",
    "    \"MDS\": \"1.5\",  # Myelodysplastic Syndrome\n",
    "    \"CML\": \"1.5\",  # Chronic Myeloid Leukemia\n",
    "    \"Other acute leukemia\": \"1\", # Other acute leukemia types\n",
    "    \"MPN\": \"2\",  # Myeloproliferative Neoplasms\n",
    "    \"HD\": \"2.5\",  # Hodgkin Disease (Lymphoma)\n",
    "    \"NHL\": \"2.5\",  # Non-Hodgkin Lymphoma\n",
    "    \"PCD\": \"2\",  # Plasma Cell Disorders (e.g., multiple myeloma)\n",
    "    \"SAA\": \"3\", # Severe Aplastic Anemia\n",
    "    \"AI\": \"3\",  # Autoimmune Disorders\n",
    "    \"HIS\": \"3\", # Hemophagocytic Immune Syndromes\n",
    "    \"IMD\": \"4.5\",  # Inherited Metabolic Disorders\n",
    "    \"IPA\": \"4.5\",  # Inherited Primary Immunodeficiency\n",
    "    \"IEA\": \"4.5\",  # Inherited Erythrocyte Abnormalities\n",
    "    \"Solid tumor\": \"4\",  # Non-blood cancers requiring HCT\n",
    "    \"IIS\": \"4.5\"  # Inherited Immune System Disorders\n",
    "}\n",
    "\n",
    "disease_num_mapping = {\n",
    "    \"ALL\": 0.7,\n",
    "    \"AML\": 0.7,\n",
    "    \"MDS\": 0.75,\n",
    "    \"CML\": 1,\n",
    "    \"Other leukemia\": 0.3,\n",
    "    \"Other acute leukemia\": 0.6,\n",
    "    \"MPN\": 0.7,\n",
    "    \"HD\": 0.3,\n",
    "    \"NHL\": 0.6,\n",
    "    \"PCD\": 0.3,\n",
    "    \"SAA\": 0.4,\n",
    "    \"AI\": 0.8,\n",
    "    \"HIS\": 0.1,\n",
    "    \"IMD\": 0.2,\n",
    "    \"IPA\": 0.85,\n",
    "    \"IEA\": 0.2,\n",
    "    \"Solid tumor\": 0.2,\n",
    "    \"IIS\": 0\n",
    "}\n",
    "\n",
    "tce_mapping = {\n",
    "    \"P/P\": 4,  # Best match\n",
    "    \"G/G\": 3,\n",
    "    \"H/H\": 3,\n",
    "    \"G/B\": 2,\n",
    "    \"H/B\": 2,\n",
    "    \"P/H\": 1,\n",
    "    \"P/B\": 0.5,\n",
    "    \"P/G\": 0   # Worst match\n",
    "}\n",
    "\n",
    "tce_match_mapping = {\n",
    "    \"Permissive\": 0.5,            \n",
    "    \"GvH non-permissive\": 1,    \n",
    "    \"Fully matched\": 0,          \n",
    "    \"HvG non-permissive\": 1       \n",
    "}\n",
    "\n",
    "tce_div_match_mapping = {\n",
    "    \"Permissive mismatched\": 0.7,            \n",
    "    \"GvH non-permissive\": 1,               \n",
    "    \"HvG non-permissive\":1,     \n",
    "    \"Bi-directional non-permissive\":2      \n",
    "}\n",
    "\n",
    "mel_mapping = {\n",
    "    \"N/A, Mel not given\": 0, \n",
    "    np.nan: 0,\n",
    "    \"MEL\": 1\n",
    "}\n",
    "\n",
    "graft_prod_mapping = {\n",
    "    \"Peripheral blood_PB\": 1,\n",
    "    \"Bone marrow_BM\": 0,          \n",
    "    \"Peripheral blood_BM\":0.5,      \n",
    "    \"Bone marrow_PB\": 0.5          \n",
    "}\n",
    "\n",
    "pulm_mapping = {\n",
    "    \"No\": 0,                \n",
    "    \"Yes\": 1,           \n",
    "    \"Not done\":0.5,     \n",
    "    np.nan:0.5\n",
    "}\n",
    "\n",
    "gvhd_proph_mapping = {\n",
    "    # **CNI-Based (Tacrolimus - FK)**\n",
    "    \"FK+- others(not MMF,MTX)\": \"0\",\n",
    "    \"FK+ MMF +- others\": \"0\",\n",
    "    \"FK+ MTX +- others(not MMF)\": \"1\",\n",
    "    \"FK alone\": \"0\",\n",
    "\n",
    "    # **CNI-Based (Cyclosporine - CSA)**\n",
    "    \"CSA alone\": \"0\",\n",
    "    \"CSA +- others(not FK,MMF,MTX)\": \"0\",\n",
    "    \"CSA + MMF +- others(not FK)\": \"0\",\n",
    "    \"CSA + MTX +- others(not MMF,FK)\": \"0\",\n",
    "\n",
    "    # **Post-Transplant Cyclophosphamide (PTCy)**\n",
    "    \"Cyclophosphamide alone\": \"1\",\n",
    "    \"Cyclophosphamide +- others\": \"1\",\n",
    "\n",
    "    # **T-Cell Depletion Strategies**\n",
    "    \"TDEPLETION alone\": \"2\",\n",
    "    \"TDEPLETION +- other\": \"2\",\n",
    "    \"CDselect alone\": \"2\",\n",
    "    \"CDselect +- other\": \"2\",\n",
    "\n",
    "    # **No Prophylaxis / Minimal Immunosuppression**\n",
    "    \"No GvHD Prophylaxis\": \"3\",\n",
    "    \"Parent Q = yes, but no agent\": \"3\",\n",
    "\n",
    "    # **Other / Unclassified**\n",
    "    \"Other GVHD Prophylaxis\": \"4\"\n",
    "}\n",
    "\n",
    "mrd_mapping = {\n",
    "    'Positive':1,\n",
    "    'Negative':0,\n",
    "    np.nan: 0.5\n",
    "}\n",
    "# Define CMV risk mapping\n",
    "cmv_mapping = {\n",
    "    \"+/+\": 1,\n",
    "    \"-/+\": 0.5, \n",
    "    \"+/-\": 0.5,\n",
    "    \"-/-\": 1  \n",
    "}\n",
    "\n",
    "donor_related_mapping = {\n",
    "    'Unrelated': 1,\n",
    "    'Related': 0,\n",
    "    'Multiple donor (non-UCB)': 0.8,\n",
    "    np.nan: 0.5,\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc8b46-423a-47ec-9b08-73dc9c5f2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train[\"tce_match_numeric\"] = train[\"tce_match\"].map(tce_match_mapping)\n",
    "train['prim_disease_hct_encoded'] = train['prim_disease_hct'].map(disease_mapping)\n",
    "train['prim_disease_hct_num_encoded'] = train['prim_disease_hct'].map(disease_num_mapping)\n",
    "train['conditioning_intensity_encoded'] = train['conditioning_intensity'].map(conditioning_mapping)\n",
    "train['cyto_score_encoded'] = train['cyto_score'].map(cyto_mapping)\n",
    "\n",
    "train['dri_score_encoded'] = train['dri_score'].map(dri_mapping)\n",
    "train['graft_type_prod_type_encoded'] = (train[\"graft_type\"] + \"_\" + train[\"prod_type\"]).map(graft_prod_mapping)\n",
    "\n",
    "train['dri_score_comorbidity_score'] = train['dri_score_encoded'] * train[\"comorbidity_score\"]\n",
    "train['cyto_score_comorbidity_score'] = train['cyto_score_encoded'] * train[\"comorbidity_score\"]\n",
    "\n",
    "\n",
    "hla_match_cols = [\"hla_match_a_high\", \"hla_match_b_high\", \"hla_match_c_high\",\n",
    "                  \"hla_match_drb1_high\", \"hla_match_dqb1_high\"]\n",
    "train[\"hla_total_match\"] = train[hla_match_cols].sum(axis=1)\n",
    "\n",
    "hla_mismatch_cols = [\"hla_match_a_low\", \"hla_match_b_low\", \"hla_match_c_low\",\n",
    "                     \"hla_match_drb1_low\", \"hla_match_dqb1_low\"]\n",
    "train[\"hla_total_mismatch\"] = train[hla_mismatch_cols].sum(axis=1)\n",
    "\n",
    "\n",
    "train['prim_disease_hct_num_encoded_cond_intensity'] = train['prim_disease_hct_num_encoded']*train[\"conditioning_intensity_encoded\"]\n",
    "train['comor_cond_intensity'] = train['comorbidity_score']*train[\"conditioning_intensity_encoded\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902c28d6-bc80-4622-9e3f-2f94e414c8f4",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a23fbc",
   "metadata": {
    "papermill": {
     "duration": 0.016426,
     "end_time": "2024-12-09T00:45:38.020919",
     "exception": false,
     "start_time": "2024-12-09T00:45:38.004493",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "RMV = [\"ID\",\"efs\",\"efs_time\",\"y\"]\n",
    "FEATURES = [c for c in train.columns if not c in RMV]\n",
    "print(f\"There are {len(FEATURES)} FEATURES: {FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da39d6af",
   "metadata": {
    "papermill": {
     "duration": 0.09673,
     "end_time": "2024-12-09T00:45:38.126278",
     "exception": false,
     "start_time": "2024-12-09T00:45:38.029548",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill nan\n",
    "\n",
    "CATS = []\n",
    "for c in FEATURES:\n",
    "    if train[c].dtype==\"object\":\n",
    "        CATS.append(c)\n",
    "        train[c] = train[c].fillna(\"NAN\") \n",
    "\n",
    "print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a988f",
   "metadata": {
    "papermill": {
     "duration": 0.1459,
     "end_time": "2024-12-09T00:45:38.281422",
     "exception": false,
     "start_time": "2024-12-09T00:45:38.135522",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Label Encode The categorical Features\n",
    "combined = train\n",
    "\n",
    "print(\"We LABEL ENCODE the CATEGORICAL FEATURES: \",end=\"\")\n",
    "\n",
    "for c in FEATURES:\n",
    "\n",
    "    # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "    if c in CATS:\n",
    "        print(f\"{c}, \",end=\" \")\n",
    "        combined[c] = combined[c].map(category_mappings[c])\n",
    "\n",
    "        combined[c] = combined[c].astype(\"int32\")\n",
    "        combined[c] = combined[c].astype(\"category\")\n",
    "\n",
    "    # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n",
    "    else:\n",
    "        if combined[c].dtype==\"float64\":\n",
    "            combined[c] = combined[c].astype(\"float32\")\n",
    "        if combined[c].dtype==\"int64\":\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "\n",
    "\n",
    "train = combined.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c034c9-5412-465b-84e4-4fa5f9d5b450",
   "metadata": {},
   "source": [
    "- More than one race 0\n",
    "- Asian 1\n",
    "- White 2\n",
    "- American Indian or Alaska Native 3\n",
    "- Native Hawaiian or other Pacific Islander 4\n",
    "- Black or African-American 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216e960-598a-4498-af90-0e4650083375",
   "metadata": {},
   "source": [
    "# Target Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466ff3d8-883b-4fa6-9909-0ebe8508b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class exponential:\n",
    "    def __init__(self, lambda_, time_col=\"efs_time\"):\n",
    "        self.lambda_ = lambda_\n",
    "        self.time_col = time_col\n",
    "\n",
    "    def predict(self, df):\n",
    "        y = self.lambda_ * np.exp(-self.lambda_ * df[self.time_col]).values\n",
    "        y = y/y.max()\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "\n",
    "class log_logistic:\n",
    "    def __init__(self, alpha, beta, time_col=\"efs_time\"):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.time_col = time_col\n",
    "    \n",
    "    def predict(self, df):\n",
    "        df = df.reset_index(drop=True).copy()\n",
    "        \n",
    "        # Log Logistic survival function\n",
    "    \n",
    "        y = 1 / (1 + (df[self.time_col] / (self.alpha)) ** (self.beta))\n",
    "\n",
    "        return y.values\n",
    "\n",
    "    def reset(self):\n",
    "        return\n",
    "        \n",
    "def preprocess_data(x_train, x_valid, wb_list, beta, alpha=1, theta = 1):\n",
    "    race_group = [0, 1, 2, 3, 4, 5]\n",
    "    \n",
    "    for race, wb in zip(race_group, wb_list):\n",
    "      #  print(wb.predict(x_train[x_train[\"race_group\"] == race].copy()))\n",
    "        y_train_wb = wb.predict(x_train[x_train[\"race_group\"] == race].copy())\n",
    "        y_valid_wb = wb.predict(x_valid[x_valid[\"race_group\"] == race].copy())\n",
    "        \n",
    "        x_train.loc[x_train[\"race_group\"] == race, \"y\"] = y_train_wb\n",
    "        x_valid.loc[x_valid[\"race_group\"] == race, \"y\"] = y_valid_wb\n",
    "        \n",
    "    for dataset in [x_train, x_valid]:\n",
    "        dataset.loc[dataset[\"efs\"] == 0, \"y\"] = 1e-5\n",
    "        dataset.loc[dataset[\"efs\"] == 1, \"y\"] += 1e-5\n",
    "    \n",
    "    # Ensure beta is non-negative\n",
    "    beta = abs(beta)\n",
    "\n",
    "    y_train = alpha * x_train[\"y\"] - beta * x_train[\"efs_time_norm\"] + theta\n",
    "    y_valid = alpha * x_valid[\"y\"] - beta * x_valid[\"efs_time_norm\"] + theta\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    drop_cols = [\"y\", \"efs_time\", \"efs\", \"efs_time_norm\"]\n",
    "    x_train.drop(columns=drop_cols, inplace=True)\n",
    "    x_valid.drop(columns=drop_cols, inplace=True)\n",
    "    \n",
    "    return x_train, y_train, x_valid, y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd2011",
   "metadata": {
    "papermill": {
     "duration": 0.376792,
     "end_time": "2024-12-09T00:45:38.683627",
     "exception": false,
     "start_time": "2024-12-09T00:45:38.306835",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost as xgb\n",
    "print(\"Using XGBoost version\",xgb.__version__)\n",
    "\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import catboost as cb\n",
    "print(\"Using CatBoost version\",cb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ac9a6-0a3b-4e66-9891-2be6908ecf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb(model_xgb, type, TRAIN = True, random_state=42, FOLDS = 10, aug = True):\n",
    "\n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=random_state)\n",
    "    np.random.seed(random_state)  \n",
    "    random.seed(random_state)\n",
    "    oof_xgb = np.zeros(len(train))\n",
    "    pred_xgb = np.zeros(len(test))\n",
    "    max_efs_time = train[\"efs_time\"].max()\n",
    "    train[\"efs_time_norm\"] = train[\"efs_time\"]/max_efs_time\n",
    "\n",
    "    nan_columns = train.columns[train.isna().any()].tolist()\n",
    "\n",
    "    if type == \"weibull\":\n",
    "        scale_1 = np.random.normal(loc=8, scale= 0.2)\n",
    "        shape_1 = np.random.normal(loc=1.8, scale= 0.2)\n",
    "    \n",
    "        scale_2 = np.random.normal(loc=5.8, scale= 0.2)\n",
    "        shape_2 = np.random.normal(loc=3.2, scale= 0.2)\n",
    "    \n",
    "        scale_3 = np.random.normal(loc=7.2, scale= 0.2)\n",
    "        shape_3 = np.random.normal(loc=2.8, scale= 0.2)\n",
    "        \n",
    "        scale_4 = np.random.normal(loc=8,   scale= 0.2)\n",
    "        shape_4 = np.random.normal(loc=1.8, scale= 0.2)\n",
    "    \n",
    "        scale_5 = np.random.normal(loc=7.8, scale= 0.2)\n",
    "        shape_5 = np.random.normal(loc=2.2, scale= 0.2)\n",
    "    \n",
    "        scale_6 = np.random.normal(loc=7.8, scale= 0.2)\n",
    "        shape_6 = np.random.normal(loc=1.8, scale= 0.2)\n",
    "    \n",
    "    \n",
    "        wb_list = [\n",
    "            weibull(scale=scale_1, shape=shape_1),\n",
    "            weibull(scale=scale_2, shape=shape_2),\n",
    "            weibull(scale=scale_3, shape=shape_3),\n",
    "            weibull(scale=scale_4, shape=shape_4),\n",
    "            weibull(scale=scale_5, shape=shape_5),\n",
    "            weibull(scale=scale_6, shape=shape_6),\n",
    "        ]\n",
    "        beta = 0.01\n",
    "        \n",
    "    if type == \"log_logistic\":\n",
    "        alpha_list = np.random.normal(loc=[6, 6, 7, 6, 8, 8], scale=0.3, size=6)\n",
    "        beta_list = np.random.normal(loc=[6, 10, 4, 10, 10, 10], scale=0.3, size=6)\n",
    "        # Create the Weibull distributions\n",
    "        wb_list = [log_logistic(alpha=s, beta=sh) for s, sh in zip(alpha_list, beta_list)]\n",
    "        beta = 0.01\n",
    "        \n",
    "    if type == \"exp\":\n",
    "        lambda_list = np.random.normal(loc=[0.20, 0.35, 0.2, 0.2, 0.15, 0.2], scale = 0.01, size = 6)\n",
    "        wb_list = [exponential(lambda_=lamb) for lamb in lambda_list]\n",
    "        beta = 0.01\n",
    "        \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "        \n",
    "        print(\"#\"*25)\n",
    "        print(f\"### Fold {i+1}\")\n",
    "        print(\"#\"*25)\n",
    "\n",
    "        if aug:\n",
    "            df_train_copy = train.loc[train_index,:].copy()\n",
    "            df_train = train.loc[train_index,:].copy()\n",
    "            df_train_aug = df_train_copy.copy()\n",
    "\n",
    "            std_dev = 2\n",
    "            random_samples = df_train_aug[(df_train_aug[\"efs\"] == 1)].sample(n=5000, replace=False, random_state=random_state).copy()\n",
    "               \n",
    "            random_variance = np.random.normal(loc=random_samples[\"efs_time\"], scale=std_dev, size=random_samples.shape[0])\n",
    "            random_variance = np.maximum(random_variance, 1e-1)\n",
    "\n",
    "            random_samples[\"efs_time\"] = random_variance\n",
    "            random_samples[\"efs_time_norm\"] = random_samples[\"efs_time\"]/max_efs_time\n",
    "                \n",
    "            df_train = pd.concat([df_train, random_samples], axis=0).reset_index(drop=True)\n",
    "\n",
    "        else:\n",
    "            df_train = train.loc[train_index,:].copy()\n",
    "\n",
    "        x_train = df_train[FEATURES+[\"efs_time\", \"efs\", \"efs_time_norm\"]].copy()\n",
    "        \n",
    "        x_valid = train.loc[test_index,FEATURES+[\"efs_time\", \"efs\", \"efs_time_norm\"]].copy()\n",
    "                    \n",
    "        x_train, y_train, x_valid, y_valid = preprocess_data(x_train.copy(), x_valid.copy(), wb_list, beta=0.01, alpha=10, theta = 1)\n",
    "\n",
    "        if TRAIN:   \n",
    "            print(\"Size:\", len(x_train))\n",
    "            print(\"N_FEATURE: \", len(x_train.columns))\n",
    "            \n",
    "            model_xgb.fit(\n",
    "                x_train, y_train,\n",
    "                eval_set=[(x_valid, y_valid)],\n",
    "                verbose=500\n",
    "            )\n",
    "            joblib.dump(model_xgb, f\"model/xgboost/xgb_model_{type}_r{random_state}_f{i}.pkl\")\n",
    "        else:\n",
    "            model_xgb = joblib.load(f\"model/xgboost/xgb_model_{type}_r{random_state}_f{i}.pkl\")\n",
    "        \n",
    "        oof_xgb[test_index] = model_xgb.predict(x_valid)\n",
    "\n",
    "    pred_xgb = None\n",
    "    y_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "    y_pred = train[[\"ID\"]].copy()\n",
    "    y_pred[\"prediction\"] = oof_xgb\n",
    "    m, cv_list = score(y_true.copy(), y_pred.copy(), \"ID\", PRINT = True)\n",
    "    print(f\"\\nOverall CV for XGBoost =\",m)\n",
    "    \n",
    "    np.save(f'model/xgb_cv/cv_{type}_r{random_state}.npy', cv_list)\n",
    "\n",
    "    return oof_xgb, pred_xgb, m, cv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea52eee-3ca1-4c16-b2de-7754490aad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_xgb(type, random_states, TRAIN = True):\n",
    "    model_xgb = XGBRegressor(\n",
    "        objective=\"reg:tweedie\",\n",
    "        device=\"cuda\",\n",
    "        max_depth=6,\n",
    "        colsample_bytree=0.2,\n",
    "        subsample=0.8,\n",
    "        n_estimators=50000,\n",
    "        reg_lambda = 2,\n",
    "        learning_rate=0.01,\n",
    "        min_child_weight=160,\n",
    "        enable_categorical = True,\n",
    "        early_stopping_rounds=500,\n",
    "        max_cat_to_onehot = 11,\n",
    "    )\n",
    "    \n",
    "    # Initialize empty lists to store results\n",
    "    oof_xgb = []\n",
    "    pred_xgb = []\n",
    "    cv_xgb =  []\n",
    "\n",
    "    AUG = True\n",
    "    for random_state in random_states:\n",
    "        \n",
    "        print(\"*\" * 100)\n",
    "        print(\"random state:\", random_state)\n",
    "        print(\"*\" * 100)\n",
    "\n",
    "        if random_state >= 20:\n",
    "            AUG = False\n",
    "            \n",
    "        oof, pred, cv, cv_list = train_xgb(\n",
    "            model_xgb,\n",
    "            type = type,\n",
    "            random_state=random_state,\n",
    "            aug= True,\n",
    "            TRAIN = TRAIN,\n",
    "            FOLDS = 10\n",
    "        )\n",
    "            \n",
    "        oof_xgb.append(oof)\n",
    "        pred_xgb.append(pred)\n",
    "        cv_xgb.append(cv_list)\n",
    "    \n",
    "        # Convert cv list to array for easier manipulation\n",
    "        cv_xgb_array = np.array(cv_xgb)\n",
    "        \n",
    "        # Define weights for each race group\n",
    "        weights_list = []\n",
    "        for race_i in range(6):  # Assuming 5 race groups\n",
    "            race_cv_scores = cv_xgb_array[:, race_i]\n",
    "            weights_list.append(rankdata(race_cv_scores))  \n",
    "        \n",
    "        # Convert weights list to array\n",
    "        weights_array = np.array(weights_list)\n",
    "        \n",
    "        # Normalize weights for each race\n",
    "        weights_array /= np.sum(weights_array, axis=1, keepdims=True)\n",
    "        \n",
    "        # Prepare data for predictions\n",
    "        y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = train[[\"ID\"]].copy()\n",
    "        \n",
    "        # Calculate weighted ensemble predictions\n",
    "        all_ranks = []\n",
    "        \n",
    "        for weights, oof in zip(weights_array.T, oof_xgb):  # Transposing to match dimensions\n",
    "            weighted_oof = np.zeros_like(oof)\n",
    "        \n",
    "            for race_i in range(6):\n",
    "                mask = train[\"race_group\"] == race_i\n",
    "                weighted_oof[mask] = weights[race_i] * (oof[mask] - oof[mask].mean()) / oof[mask].std()\n",
    "            \n",
    "            all_ranks.append(weighted_oof)\n",
    "        \n",
    "        # Convert to numpy array and take mean across all models\n",
    "        all_ranks = np.array(all_ranks)\n",
    "        y_pred[\"prediction\"] = np.mean(all_ranks, axis=0)\n",
    "        \n",
    "        # Evaluate ensemble performance\n",
    "        m, _ = score(y_true.copy(), y_pred.copy(), \"ID\", PRINT = True)\n",
    "        print(f\"\\nOverall CV for Ensemble = {m}\")\n",
    "\n",
    "    return oof_xgb, pred_xgb, cv_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1fdda8-b1f8-425a-b70f-f4a423e6c5d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_states = [r for r in range(0, 40)]\n",
    "\n",
    "oof_xgb_wb, pred_xgb_wb, cv_xgb_wb = train_loop_xgb(\"weibull\", random_states, TRAIN = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe88f1",
   "metadata": {
    "papermill": {
     "duration": 0.012544,
     "end_time": "2024-12-09T00:46:32.244878",
     "exception": false,
     "start_time": "2024-12-09T00:46:32.232334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CatBoost\n",
    "We train CatBoost model for 10 folds and achieve **CV 0.674**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d33575-6b3c-442a-833f-b27c5552a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train_catboost(random_state=42, FOLDS = 10, aug = True, TRAIN = True):\n",
    "\n",
    "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=random_state)\n",
    "    np.random.seed(random_state)  \n",
    "    random.seed(random_state)\n",
    "    oof_cat = np.zeros(len(train))\n",
    "    pred_cat = np.zeros(len(test))\n",
    "    max_efs_time = train[\"efs_time\"].max()\n",
    "    train[\"efs_time_norm\"] = train[\"efs_time\"]/max_efs_time\n",
    "   \n",
    "    race_group = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "    nan_columns = train.columns[train.isna().any()].tolist()\n",
    "    \n",
    "    scale_1 = np.random.normal(loc=8, scale= 0.1)\n",
    "    shape_1 = np.random.normal(loc=1.8, scale= 0.1)\n",
    "\n",
    "    scale_2 = np.random.normal(loc=5.8, scale= 0.1)\n",
    "    shape_2 = np.random.normal(loc=3.2, scale= 0.1)\n",
    "\n",
    "    scale_3 = np.random.normal(loc=7.2, scale= 0.1)\n",
    "    shape_3 = np.random.normal(loc=2.8, scale= 0.1)\n",
    "    \n",
    "    scale_4 = np.random.normal(loc=8,   scale= 0.1)\n",
    "    shape_4 = np.random.normal(loc=1.8, scale= 0.1)\n",
    "\n",
    "    scale_5 = np.random.normal(loc=7.8, scale= 0.1)\n",
    "    shape_5 = np.random.normal(loc=2.2, scale= 0.1)\n",
    "\n",
    "    scale_6 = np.random.normal(loc=7.8, scale= 0.1)\n",
    "    shape_6 = np.random.normal(loc=1.8, scale= 0.1)\n",
    "    \n",
    "    wb_list = [\n",
    "        weibull(scale=scale_1, shape=shape_1),\n",
    "        weibull(scale=scale_2, shape=shape_2),\n",
    "        weibull(scale=scale_3, shape=shape_3),\n",
    "        weibull(scale=scale_4, shape=shape_4),\n",
    "        weibull(scale=scale_5, shape=shape_5),\n",
    "        weibull(scale=scale_6, shape=shape_6),\n",
    "    ]\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "        if TRAIN:\n",
    "            print(\"#\"*25)\n",
    "            print(f\"### Fold {i+1}\")\n",
    "            print(\"#\"*25)\n",
    "\n",
    "\n",
    "        if aug:\n",
    "            df_train_copy = train.loc[train_index,:].copy()\n",
    "            df_train = train.loc[train_index,:].copy()\n",
    "            df_train_aug = df_train_copy.copy()\n",
    "\n",
    "            std_dev = 1.5\n",
    "            random_samples = df_train_aug[(df_train_aug[\"efs\"] == 1)].sample(n=5000, replace=False, random_state=random_state).copy()\n",
    "               \n",
    "              #  random_samples = df_train_aug.sample(n=int(len(df_train_aug)), replace=False, random_state=random_state)\n",
    "            random_variance = np.random.normal(loc=random_samples[\"efs_time\"], scale=std_dev, size=random_samples.shape[0])\n",
    "            random_variance = np.maximum(random_variance, 1e-1)\n",
    "\n",
    "            random_samples[\"efs_time\"] = random_variance\n",
    "            random_samples[\"efs_time_norm\"] = random_samples[\"efs_time\"]/max_efs_time                    \n",
    "\n",
    "            df_train = pd.concat([df_train, random_samples], axis=0).reset_index(drop=True)\n",
    "        else:\n",
    "            df_train = train.loc[train_index,:].copy()\n",
    "\n",
    "        x_train = df_train[FEATURES+[\"efs_time\", \"efs\", \"efs_time_norm\"]].copy()\n",
    "        x_valid = train.loc[test_index,FEATURES+[\"efs_time\", \"efs\", \"efs_time_norm\"]].copy()\n",
    "\n",
    "        x_train, y_train, x_valid, y_valid = preprocess_data(x_train.copy(), x_valid.copy(), wb_list, beta=0.01, alpha=0.5, theta = 0.1)\n",
    "\n",
    "        if TRAIN:\n",
    "            print(\"Size:\", len(x_train))\n",
    "            print(\"N_FEATURE: \", len(x_train.columns))\n",
    "\n",
    "        # Create a CatBoost regressor with additional parameters\n",
    "        if TRAIN:\n",
    "            model_cat = CatBoostRegressor(\n",
    "                task_type=\"GPU\",\n",
    "                bootstrap_type = \"Bernoulli\",\n",
    "                grow_policy=\"Lossguide\",\n",
    "                learning_rate=0.01,\n",
    "                iterations=50000,\n",
    "                depth=8,\n",
    "                one_hot_max_size=6,\n",
    "                cat_features=CATS,\n",
    "                early_stopping_rounds=250,\n",
    "                loss_function=\"Tweedie:variance_power=1.5\",\n",
    "                min_data_in_leaf=60,\n",
    "            )\n",
    "    \n",
    "            model_cat.fit(x_train,y_train,\n",
    "                      eval_set=(x_valid, y_valid),\n",
    "                      cat_features=CATS,\n",
    "                      verbose=0)\n",
    "            \n",
    "            #model_cat.save_model(\"catboost_model.cbm\")\n",
    "            joblib.dump(model_cat, f\"model/catboost/cat_model_r{random_state}_f{i}.pkl\")\n",
    "        else:\n",
    "            model_cat = joblib.load(f\"model/catboost/cat_model_r{random_state}_f{i}.pkl\")\n",
    "            \n",
    "        # INFER OOF\n",
    "        oof_cat[test_index] = model_cat.predict(x_valid)\n",
    "        # INFER TEST\n",
    "      #  pred_cat += model_cat.predict(x_test)\n",
    "\n",
    "    if TRAIN:\n",
    "        # COMPUTE AVERAGE TEST PREDS\n",
    "        pred_cat = None\n",
    "        y_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "        y_pred = train[[\"ID\"]].copy()\n",
    "        y_pred[\"prediction\"] = oof_cat\n",
    "        m, cv_list = score(y_true.copy(), y_pred.copy(), \"ID\", PRINT = True)\n",
    "        print(f\"\\nOverall CV for CatBoost =\",m)\n",
    "    \n",
    "        np.save(f'model/cat_cv/cv_r{random_state}.npy', cv_list)\n",
    "\n",
    "    else:\n",
    "        cv_list = np.load(f'model/cat_cv/cv_r{random_state}.npy', allow_pickle=True)\n",
    "        m = np.mean(cv_list) - np.std(cv_list)\n",
    "    return oof_cat, pred_cat, m, cv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3838802-342e-4392-bc38-8e1d1da328fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_cat(random_states, TRAIN = True):\n",
    "    # Initialize empty lists to store results\n",
    "    oof_cat = []\n",
    "    pred_cat = []\n",
    "    cv_cat = []\n",
    "\n",
    "    AUG = True\n",
    "    for random_state in random_states:\n",
    "        print(\"*\" * 100)\n",
    "        print(\"random state:\", random_state)\n",
    "        print(\"*\" * 100)\n",
    "\n",
    "        if random_state > 60:\n",
    "            AUG = False\n",
    "            \n",
    "        oof, pred, cv, cv_list = train_catboost(\n",
    "            random_state=random_state,\n",
    "            aug=AUG,\n",
    "            TRAIN = TRAIN\n",
    "        )\n",
    "        oof_cat.append(oof)\n",
    "        pred_cat.append(pred)\n",
    "        cv_cat.append(cv_list)\n",
    "    \n",
    "        # Convert cv list to array for easier manipulation\n",
    "        cv_cat_array = np.array(cv_cat)\n",
    "        \n",
    "        # Define weights for each race group\n",
    "        weights_list = []\n",
    "        for race_i in range(6):  # Assuming 5 race groups\n",
    "            race_cv_scores = cv_cat_array[:, race_i]\n",
    "            weights_list.append(rankdata(race_cv_scores))  \n",
    "        \n",
    "        # Convert weights list to array\n",
    "        weights_array = np.array(weights_list)\n",
    "        \n",
    "        # Normalize weights for each race\n",
    "        weights_array /= np.sum(weights_array, axis=1, keepdims=True)\n",
    "        \n",
    "        # Prepare data for predictions\n",
    "        y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "        y_pred = train[[\"ID\"]].copy()\n",
    "        \n",
    "        # Calculate weighted ensemble predictions\n",
    "        all_ranks = []\n",
    "        for weights, oof in zip(weights_array.T, oof_cat):  # Transposing to match dimensions\n",
    "            weighted_oof = np.zeros_like(oof)\n",
    "        \n",
    "            for race_i in range(6):\n",
    "                mask = train[\"race_group\"] == race_i\n",
    "                weighted_oof[mask] = weights[race_i] * (oof[mask] - oof[mask].mean()) / oof[mask].std()\n",
    "            \n",
    "            all_ranks.append(weighted_oof)\n",
    "        \n",
    "        # Convert to numpy array and take mean across all models\n",
    "        all_ranks = np.array(all_ranks)\n",
    "        y_pred[\"prediction\"] = np.mean(all_ranks, axis=0)\n",
    "        \n",
    "        # Evaluate ensemble performance\n",
    "        m, cv_xgb = score(y_true.copy(), y_pred.copy(), \"ID\", PRINT = True)\n",
    "        print(f\"\\nOverall CV for Ensemble = {m}\")\n",
    "\n",
    "\n",
    "    return oof_cat, pred_cat, cv_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0830ce2-fb3f-45e2-8055-d794183a9c69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_states = [r for r in range(40, 80)]\n",
    "\n",
    "oof_cat, pred_cat, cv_cat = train_loop_cat(random_states, TRAIN=True) "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    },
    {
     "sourceId": 211253469,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 211322530,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 870.682815,
   "end_time": "2024-12-09T00:56:37.628807",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-09T00:42:06.945992",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
